{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import S\n",
    "from fastapi.params import Query\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from dotenv import load_dotenv\n",
    "import os, sys, json, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка в процессе обновления для potok_kumi: DATA_TIKTOK\n"
     ]
    }
   ],
   "source": [
    "def update_info_in_sheets():\n",
    "    \"\"\"\n",
    "    Updates the text in cell H1 to 'music info' for each user's Google Sheet.\n",
    "    \"\"\"\n",
    "    with open(\"db/main/db.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        table_list = json.load(file)\n",
    "\n",
    "    for index, account in enumerate(table_list[\"accounts\"]):\n",
    "        if index > 14:\n",
    "            try:\n",
    "                # Set up Google Sheets API credentials\n",
    "                scope = [\n",
    "                    \"https://spreadsheets.google.com/feeds\",\n",
    "                    \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "                    \"https://www.googleapis.com/auth/drive.file\",\n",
    "                    \"https://www.googleapis.com/auth/drive\"\n",
    "                ]\n",
    "                credentials = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "                    'static/reelstranscription-a94a4b07252e.json', scope)\n",
    "                client = gspread.authorize(credentials)\n",
    "\n",
    "                # Open the Google Sheet\n",
    "                google_sheet_url = f'https://docs.google.com/spreadsheets/d/{account[\"table_id\"]}/edit?usp=sharing'\n",
    "\n",
    "                #-------------------\n",
    "\n",
    "\n",
    "                #sheet = client.open_by_url(google_sheet_url).worksheet('DATA')\n",
    "\n",
    "                # Read and print the current value of cell H1\n",
    "                # current_value = sheet.acell('H1').value\n",
    "                # print(f\"Current value in H1 for {index} -  {account['username']}: {current_value}\")\n",
    "\n",
    "                # # Update cell H1\n",
    "                # sheet.batch_update([\n",
    "                #     {\"range\": \"D1\", \"values\": [[\"viewsFilter\"]]}\n",
    "                # ])\n",
    "                # print(f\"Updated D1 in instagram for {account['username']}\")\n",
    "                # time.sleep(2)\n",
    "\n",
    "                # sheet = client.open_by_url(google_sheet_url).worksheet('DATA_TIKTOK')\n",
    "\n",
    "                # # Update cell H1\n",
    "                # sheet.batch_update([\n",
    "                #     {\"range\": \"D1\", \"values\": [[\"viewsFilter\"]]}\n",
    "                # ])\n",
    "                # print(f\"Updated D1 in tiktok for {account['username']}\")\n",
    "                # time.sleep(2)\n",
    "\n",
    "\n",
    "                #-------------------\n",
    "\n",
    "\n",
    "                # sheet = client.open_by_url(google_sheet_url).worksheet('DATA')\n",
    "                # # Получение текущих значений из столбца D, начиная с D2\n",
    "                # current_values = sheet.get('D2:D')  # Получаем все значения начиная с D2\n",
    "\n",
    "                # # Умножаем на 1000, если значение не равно 1 и не больше 1000\n",
    "                # updated_values = [[float(value[0]) * 1000] for value in current_values if value and float(value[0]) != 1 and float(value[0]) <= 1000]  \n",
    "\n",
    "                # # Обновление ячеек с новыми значениями\n",
    "                # sheet.update('D2:D', updated_values)  # Обновляем значения в диапазоне D2 и ниже\n",
    "                # print(f\"Updated D2 and below in inst for {account['username']}\")\n",
    "                # time.sleep(2)\n",
    "\n",
    "                # sheet = client.open_by_url(google_sheet_url).worksheet('DATA_TIKTOK')\n",
    "                # # Получение текущих значений из столбца D, начиная с D2\n",
    "                # current_values = sheet.get('D2:D')  # Получаем все значения начиная с D2\n",
    "\n",
    "                # # Умножаем на 1000, если значение не равно 1 и не больше 1000\n",
    "                # updated_values = [[float(value[0]) * 1000] for value in current_values if value and float(value[0]) != 1 and float(value[0]) <= 1000]  \n",
    "\n",
    "                # # Обновление ячеек с новыми значениями\n",
    "                # sheet.update('D2:D', updated_values)  # Обновляем значения в диапазоне D2 и ниже\n",
    "                # print(f\"Updated D2 and below in tiktok for {account['username']}\")\n",
    "                # time.sleep(2)\n",
    "\n",
    "                #-------------------\n",
    "\n",
    "\n",
    "                list_array = ['DATA', 'DATA_TIKTOK']\n",
    "                for list in list_array:\n",
    "                    # sheet = client.open_by_url(google_sheet_url).worksheet(list)\n",
    "                    # # Получение текущих значений из столбца D, начиная с D2\n",
    "                    # current_values = sheet.get('D2:D')  # Получаем все значения начиная с D2\n",
    "\n",
    "                    # # Обновление формата ячеек на \"Номер\"\n",
    "                    # sheet.batch_update([{\n",
    "                    #     'range': 'D2:D',\n",
    "                    #     'values': current_values,\n",
    "                    #     'format': {\n",
    "                    #         'numberFormat': {\n",
    "                    #             'type': 'NUMBER',\n",
    "                    #             'pattern': '0'  # Формат без десятичных знаков\n",
    "                    #         }\n",
    "                    #     }\n",
    "                    # }])\n",
    "\n",
    "                    # 4) Открываем нужный лист (вкладку) по названию 'DATA'\n",
    "                    worksheet = client.open_by_url(google_sheet_url).worksheet(list)\n",
    "                    \n",
    "                    # 5) Применяем форматирование к диапазону D2:D\n",
    "                    worksheet.format(\n",
    "                        \"D2:D\",  # Форматируем весь столбец D начиная со 2-й строки\n",
    "                        {\n",
    "                            \"numberFormat\": {\n",
    "                                \"type\": \"NUMBER\",\n",
    "                                # Паттерн формата: 0.00 -> число с 2 знаками после запятой\n",
    "                                # Можно указать \"0\" для целых или другое (например \"0.000\")\n",
    "                                \"pattern\": \"#,##0\"\n",
    "                            }\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Ошибка в процессе обновления для {account['username']}: {e}\")\n",
    "\n",
    "update_info_in_sheets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1) Указываем скоупы: работа с Google Sheets и Drive.\n",
    "    scopes = [\n",
    "        \"https://spreadsheets.google.com/feeds\",\n",
    "        \"https://www.googleapis.com/auth/spreadsheets\",\n",
    "        \"https://www.googleapis.com/auth/drive.file\",\n",
    "        \"https://www.googleapis.com/auth/drive\"\n",
    "    ]\n",
    "\n",
    "    # 2) Авторизуемся с помощью ключей сервис-аккаунта\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "        'credentials.json',  # путь к файлу с ключами\n",
    "        scopes\n",
    "    )\n",
    "    client = gspread.authorize(creds)\n",
    "\n",
    "    # 3) Открываем нужную гугл-таблицу по названию или по spreadsheet key\n",
    "    spreadsheet = client.open(\"Название вашей таблицы\")\n",
    "    \n",
    "    # 4) Открываем нужный лист (вкладку) по названию 'DATA'\n",
    "    worksheet = spreadsheet.worksheet(\"DATA\")\n",
    "    \n",
    "    # 5) Применяем форматирование к диапазону D2:D\n",
    "    worksheet.format(\n",
    "        \"D2:D\",  # Форматируем весь столбец D начиная со 2-й строки\n",
    "        {\n",
    "            \"numberFormat\": {\n",
    "                \"type\": \"NUMBER\",\n",
    "                # Паттерн формата: 0.00 -> число с 2 знаками после запятой\n",
    "                # Можно указать \"0\" для целых или другое (например \"0.000\")\n",
    "                \"pattern\": \"0.00\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"Форматирование выполнено успешно!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Trailing data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 35\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Main execution\u001b[39;00m\n\u001b[0;32m     28\u001b[0m json_files \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreels/test/reels_transcriptions_thread_10.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreels/test/reels_transcriptions_thread_01.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreels/test/reels_transcriptions_thread_06.json\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreels/test/reels_transcriptions_1_base.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     33\u001b[0m ]\n\u001b[1;32m---> 35\u001b[0m combined_df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_files\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_colwidth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     37\u001b[0m combined_df  \u001b[38;5;66;03m# Display the DataFrame as a table in the notebook\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m, in \u001b[0;36mcreate_table\u001b[1;34m(json_files)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_table\u001b[39m(json_files):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Load all JSON data into DataFrames and sort by 'video'\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     dataframes \u001b[38;5;241m=\u001b[39m [load_json_to_dataframe(file)\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m json_files]\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Check if all DataFrames have the same number of elements\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     num_elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataframes[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[15], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_table\u001b[39m(json_files):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Load all JSON data into DataFrames and sort by 'video'\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     dataframes \u001b[38;5;241m=\u001b[39m [\u001b[43mload_json_to_dataframe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m json_files]\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Check if all DataFrames have the same number of elements\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     num_elements \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataframes[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[15], line 4\u001b[0m, in \u001b[0;36mload_json_to_dataframe\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json_to_dataframe\u001b[39m(file_path):\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\AAA\\Projects\\trndwch\\mfai_neuro_reels_writer\\venv_win\\lib\\site-packages\\pandas\\io\\json\\_json.py:815\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options, dtype_backend, engine)\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\AAA\\Projects\\trndwch\\mfai_neuro_reels_writer\\venv_win\\lib\\site-packages\\pandas\\io\\json\\_json.py:1025\u001b[0m, in \u001b[0;36mJsonReader.read\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_object_parser(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(data_lines))\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1025\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m   1027\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mconvert_dtypes(\n\u001b[0;32m   1028\u001b[0m         infer_objects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype_backend\n\u001b[0;32m   1029\u001b[0m     )\n",
      "File \u001b[1;32mc:\\AAA\\Projects\\trndwch\\mfai_neuro_reels_writer\\venv_win\\lib\\site-packages\\pandas\\io\\json\\_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m   1049\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32mc:\\AAA\\Projects\\trndwch\\mfai_neuro_reels_writer\\venv_win\\lib\\site-packages\\pandas\\io\\json\\_json.py:1187\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 1187\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\AAA\\Projects\\trndwch\\mfai_neuro_reels_writer\\venv_win\\lib\\site-packages\\pandas\\io\\json\\_json.py:1403\u001b[0m, in \u001b[0;36mFrameParser._parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1399\u001b[0m orient \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morient\n\u001b[0;32m   1401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1402\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m DataFrame(\n\u001b[1;32m-> 1403\u001b[0m         \u001b[43mujson_loads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecise_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecise_float\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1404\u001b[0m     )\n\u001b[0;32m   1405\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m orient \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1406\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m   1407\u001b[0m         \u001b[38;5;28mstr\u001b[39m(k): v\n\u001b[0;32m   1408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m ujson_loads(json, precise_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecise_float)\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   1409\u001b[0m     }\n",
      "\u001b[1;31mValueError\u001b[0m: Trailing data"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_json_to_dataframe(file_path):\n",
    "    return pd.read_json(file_path)\n",
    "\n",
    "def create_table(json_files):\n",
    "    # Load all JSON data into DataFrames and sort by 'video'\n",
    "    dataframes = [load_json_to_dataframe(file).sort_values(by='video').reset_index(drop=True) for file in json_files]\n",
    "\n",
    "    # Check if all DataFrames have the same number of elements\n",
    "    num_elements = len(dataframes[0])\n",
    "    if not all(len(df) == num_elements for df in dataframes):\n",
    "        raise ValueError(\"All JSON files must have the same number of elements\")\n",
    "\n",
    "    # Combine DataFrames\n",
    "    combined_df = pd.DataFrame({\n",
    "        'id': range(num_elements),  # Row number\n",
    "        'videopath': dataframes[0]['video']  # Use videopath from the first file only\n",
    "    })\n",
    "\n",
    "    # Add transcriptions from each file\n",
    "    for i, df in enumerate(dataframes):\n",
    "        combined_df[f'transcription_{i}'] = df['transcription']\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Main execution\n",
    "json_files = [\n",
    "    'reels/test/reels_transcriptions_thread_10.json',\n",
    "    'reels/test/reels_transcriptions_thread_01.json',\n",
    "    'reels/test/reels_transcriptions_thread_06.json',\n",
    "    'reels/test/reels_transcriptions_1_base.json'\n",
    "]\n",
    "\n",
    "combined_df = create_table(json_files)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "combined_df  # Display the DataFrame as a table in the notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
